\documentclass[preprint,12pt]{elsarticle}
\usepackage{pznotations}


\usepackage{xcolor}


\usepackage[
    colorlinks=true,
    linkcolor=blue,    % Color for sections, figures, tables, etc. links
    citecolor=red,     % Color for bibliographic citations
    urlcolor=cyan      % Color for external URLs
]{hyperref}
\usepackage{cleveref}

\usepackage{natbib}

\newcommand{\zxy}[1]{\textcolor{purple}{zxy:#1}}


\begin{document}
\begin{frontmatter}

\title{OJOLLM: A Large Language Model-driven Framework for Multi-Objective Optimization of Offshore Jacket Structures}

\author[aff1,aff2]{Wenhao Li}
\author[aff2]{Xiaoyuan Zhang\corref{cor1}}
\cortext[cor1]{Corresponding author}
\author[aff2]{Yue Wang}
\author[aff3]{Nian Ran}
\author[aff1]{Haitao Zhu}

\address[aff1]{Tianjin University}
\address[aff2]{Zhongguancun Academy}
\address[aff3]{The University of Manchester}

\begin{abstract}
    The design of large-scale structures like offshore jackets is a demanding multi-objective optimization (MOO) problem, hampered by mixed discrete-continuous variable spaces and the inefficiency of traditional evolutionary algorithms that rely on blind, stochastic operators. Here, we introduce a new optimization paradigm, LLM-GA, which recasts a Large Language Model (LLM) as an intelligent genetic operator capable of leveraging domain knowledge. Through strategic prompt engineering, the LLM comprehends structural context, such as load paths and member connectivity, to perform context-aware crossover and mutation. Unlike random perturbations, our method generates semantically coherent and structurally plausible offspring, mimicking an expert engineer's reasoning. We validated this framework on a jacket design case, optimizing for weight, axial stress, and bending stress. The knowledge-infused LLM-GA significantly outperforms established baselines (GA, RS, MOEA/D) and a knowledge-agnostic LLM-GA variant in both convergence speed and the quality of the final Pareto front, achieving a \textbf{[e.g., X\%]} improvement in the hypervolume indicator. Our findings demonstrate that integrating LLMs as domain-aware inference engines for optimization can bridge the gap between abstract engineering knowledge and effective automated design, offering a powerful new pathway for solving complex real-world engineering problems.
\end{abstract}

\begin{keyword}
Multi-objective optimization \sep Offshore jacket \sep Large language model \sep Genetic algorithm \sep SACS
\end{keyword}

\end{frontmatter}

\section{Introduction}

The field of ocean engineering is characterized by projects of immense scale and uncompromising safety standards. Offshore jacket structures, the skeletal steel frames supporting platforms, are critical assets whose design represents a complex balancing act between economic viability (minimizing weight) and structural integrity (withstanding extreme loads). The optimization of these structures is a quintessential multi-objective, \textbf{mixed-variable} problem. Designers must simultaneously select optimal member properties from a \textit{discrete} catalog of sections (e.g., tube outer diameter and wall thickness) while also determining the optimal spatial placement of structural nodes via \textit{continuous} geometric coordinates.

Traditional approaches to this challenge have relied heavily on numerical Multi-Objective Evolutionary Algorithms (MOEAs). While powerful, these algorithms operate on an abstract vector representation of the design space, treating the sophisticated simulation software (e.g., SACS) as a ``black box.'' This paradigm lacks any intrinsic understanding of the engineering domain. Consequently, its exploration is inefficient, and its genetic operators (e.g., numerical crossover, random perturbation) are ``blind.'' This blindness is especially detrimental in mixed-variable spaces. A purely stochastic operator is incapable of making an intelligent, context-driven decision on \textit{which} variable type to modify—or how to modify \textit{both} synergistically—to solve a specific structural deficiency.

In this paper, we introduce a novel framework, the Large Language Model-driven Genetic Algorithm (LLM-GA), which represents a paradigm shift from purely numerical optimization to a semantic, knowledge-infused search. Our framework positions an LLM as a \textbf{unified intelligent operator} at the core of the evolutionary process. The LLM operates directly on a code-native representation of the design (i.e., SACS input blocks). This unified representation allows the LLM to leverage its vast, pre-trained knowledge to perform ``semantic'' genetic operations on \textit{both} discrete sections and continuous geometries. For instance, prompted with a design's performance metrics, the LLM can reason whether a high-stress failure is best fixed by a \textit{discrete} change (increasing a member's wall thickness) or a \textit{continuous} one (adjusting a bracing angle). Furthermore, we introduce a unique ``experience feedback loop'', where the LLM periodically analyzes high-performing solutions to distill strategic insights and refine its generative strategy.

The primary contributions of this work are as follows:
\begin{enumerate}
    \item We propose a novel LLM-GA framework that integrates an LLM as the central, \textit{unified} generative engine for mixed-variable optimization.
    \item We develop ``semantic'' genetic operators based on structured prompt engineering, enabling the LLM to intelligently create new candidates by reasoning about and modifying \textit{both} discrete (member sections) and continuous (joint coordinates) parameters.
    \item We design a dynamic ``experience feedback loop'' that allows the framework to achieve in-situ strategy self-improvement by prompting the LLM to learn from its previous generations.
    \item We demonstrate the \textit{effectiveness and versatility} of the LLM-GA framework on a complex SACS jacket design problem, validating its performance across three distinct optimization tasks: section-only, geometry-only, and fully-mixed, achieving \textbf{[e.g., Y\% and Z\%]} performance gains over traditional MOEAs.
\end{enumerate}

\section{Preliminaries}
\subsection{Multiobjective Optimization}
Multi-objective optimization (MOO) seeks a set of non-dominated solutions that balance conflicting objectives rather than a single optimum. A design is Pareto-optimal if no other design improves one objective without degrading at least one other. The collection of non-dominated solutions forms the Pareto front, whose quality is often assessed by indicators such as hypervolume (HV) and the spread of trade-offs across objectives. Evolutionary algorithms (e.g., NSGA-II, MOEA/D) advance the front through selection and variation, while practical engineering problems frequently mix discrete and continuous variables (e.g., catalog sections versus geometric coordinates) and must satisfy feasibility constraints such as unity checks.

\section{Related Work}

Our research is positioned at the intersection of three distinct but rapidly converging fields: (1) structural engineering optimization, (2) multi-objective evolutionary algorithms (MOEAs) for mixed-variable problems, and (3) the application of Large Language Models (LLMs) in scientific and engineering design.

\subsection{Structural Engineering Optimization}

The optimization of offshore jacket structures is a persistent challenge, driven by the dual imperatives of cost reduction (weight) and safety assurance (stress and fatigue)~\citep{savsani2021topology,zheng2023efficient}. The design space is inherently heterogeneous, mixing discrete member sizes with continuous geometric or topological parameters. Recent advancements have increasingly focused on addressing this complexity. For instance, studies have employed Particle Swarm Optimization (PSO) to optimize jacket substructures~\citep{benitez2025pso}, while others have successfully integrated complex objectives, such as fatigue and ultimate limit states, directly into the MOO framework.

Given the high computational expense of Finite Element Analysis (FEA) solvers like SACS, a dominant research thrust involves surrogate-assisted optimization. However, a persistent challenge in these state-of-the-art methods is the reliance on conventional metaheuristics. These algorithms, while effective, still treat the complex design space as a numerical vector, remaining ``blind'' to the underlying engineering principles and structural mechanics. Our work directly confronts this ``blindness'' by introducing a generative agent that possesses domain-aware reasoning capabilities.

\subsection{MOEAs for Mixed-Variable Optimization (MVO)}

Our LLM-GA framework explicitly builds upon the robust \textit{selection} principles of canonical MOEAs like NSGA-II~\citep{deb2002fast} and MOEA/D~\citep{zhang2007moead}. The primary bottleneck in applying these algorithms to real-world engineering problems is the design of effective \textit{variation} operators, especially for constrained mixed-variable optimization (CMVO). The challenge of simultaneously handling discrete, integer, and continuous variables has spurred significant dedicated research.

Recent efforts (2024-2025) have focused on developing specialized algorithms and benchmarks. For instance, new benchmark functions have been proposed to specifically test evolutionary algorithms on CMVO problems. Concurrently, new algorithms have emerged, such as decomposition-based methods with dynamic resource allocation tailored for MVO. To manage high computational costs, automated frameworks for surrogate-assisted EAs have also been proposed to reduce human intervention~\citep{dai2025automated}. While these methods advance the \textit{numerical} treatment of mixed variables, they still lack semantic understanding. Our approach posits that an LLM, as a unified operator, can bypass the need for complex, hand-crafted numerical operators by reasoning about the problem at a higher, semantic level.

Recent efforts (2024--2025) have increasingly focused on addressing the challenges of mixed-variable spaces and algorithm automation. For instance, \citet{Yu2024MOEAD} and \citet{Jia2025DGMOEAD} improved decomposition-based methods (MOEA/D) by incorporating probabilistic models and differential grouping to better handle discrete-continuous heterogeneity. Concurrently, the integration of Large Language Models (LLMs) into evolutionary computation has emerged as a transformative trend. \citet{Stein2025LLaMEA} demonstrated that LLMs can automatically generate high-performing metaheuristic operators, while \citet{Xie2025LLMSAEA} utilized LLMs to manage surrogate models for expensive optimization problems. Additionally, automated frameworks such as AutoSAPSO~\citep{Dai2025AutoSAPSO} have been proposed to reduce human intervention in algorithm design.

\subsection{Large Language Models in Science and Engineering}

The advent of powerful LLMs has catalyzed a paradigm shift, moving these models from pure language tasks to complex reasoning and generation in science and engineering. A major application area is generative design, particularly in chemistry and materials science, where LLMs generate novel molecular structures (e.g., SMILES strings) optimized for specific properties~\citep{wu2024prompt}. This is a strong analogue to our work, as both involve generating solutions within a domain-specific, syntax-constrained ``language''.

Most critically, a new frontier of research has emerged in 2024 and 2025 focusing on LLMs as \textit{agents within the optimization loop}. This includes framing optimization itself as a generative task~\citep{yang2024large} and the development of ``LLM-based Evolutionary Algorithms'' (LMEAs). Recent frameworks like REvolution (2025) integrate LLMs with evolutionary computation for hardware (RTL) code generation~\citep{min2025revolution}, while others like HSEvo (2025) use LLMs to evolve heuristics or programs~\citep{dat2025hsevo}. These studies confirm that coupling LLMs with EAs is a cutting-edge approach.

However, most applications in civil and structural engineering still position the LLM as a ``co-pilot'' to augment the human designer~\citep{alwashah2025generative,liu2025generative}. A 2025 scoping review noted that key challenges remain in autonomous design, such as ensuring solution diversity and maintaining complex system constraints~\citep{feng2025guiding}. Our work directly addresses these gaps. To our knowledge, it is the first framework to integrate an LLM as the core, \textit{autonomous generative operator} inside a closed-loop MOEA for a complex, mixed-variable \textit{structural engineering} problem, complete with a feedback loop for in-context learning and strategy refinement.

\section{Methodology}

This study introduces a novel optimization paradigm, the Large Language Model-driven Genetic Algorithm (LLM-GA), engineered for complex design optimization problems. The methodology is predicated on the integration of a Large Language Model (LLM) within a multi-objective evolutionary computation framework. This section details the architectural design, core components, and operational mechanics of the proposed system.

\subsection{Overall System Architecture}
The LLM-GA operates as a population-based, iterative optimization framework. The algorithm's primary function is to navigate a high-dimensional design space to identify a set of Pareto-optimal solutions that represent the most effective trade-offs among multiple conflicting objectives. Figure~\ref{fig:llm-ga-workflow} summarizes the closed-loop data flow between the evolutionary core, the evaluation oracle, and the LLM-guided operators. The optimization process follows a canonical evolutionary cycle:
\begin{enumerate}
    \item \textbf{Initialization:} A diverse initial population of candidate designs is established through a process that systematically mutates a set of predefined seed designs to ensure broad coverage of the solution space.
    \item \textbf{Evaluation:} Each candidate solution is evaluated by a specialized evaluation oracle, which quantifies its performance with respect to each design objective.
    \item \textbf{Selection:} A subset of high-performing and diverse individuals is selected from the current population to serve as parents for the subsequent generation. This selection is governed by the principles of the Non-dominated Sorting Genetic Algorithm II (NSGA-II).
    \item \textbf{Generation:} The selected parents are utilized to generate new offspring solutions. This crucial step is delegated to an LLM, which interprets contextual prompts to perform semantic genetic operations.
\end{enumerate}
\begin{figure}[t]
    \centering
\includegraphics[width=\linewidth]{jacket.drawio.png}
    \caption{LLM-GA workflow. The upper band depicts the optimization loop that cycles through initialization, high-fidelity SACS analysis, NSGA-II selection, and offspring generation until termination. The lower band shows the LLM-guided operators and experience feedback loop that inject semantic reasoning into the creation of new candidates.}
    \label{fig:llm-ga-workflow}
\end{figure}
This iterative process continues until a predefined termination criterion, such as the exhaustion of the computational budget, is satisfied.

\subsection{Solution Representation}
In contrast to traditional evolutionary algorithms that employ numerical vector representations (e.g., $\mathbf{x} = [\mathbf{x}_{\text{discrete}}, \mathbf{x}_{\text{continuous}}]$), a candidate solution in LLM-GA is encoded as a \textit{semantic, code-native representation}. This is a structured JSON object integral to the human-readable, context-rich communication with the LLM.

The schema is defined as a \textbf{unified container} for all optimizable parameters:
\begin{itemize}
    \item A top-level JSON object contains a single key, \texttt{"new\_code\_blocks"}.
    \item The value associated with this key is a dictionary. Each key in this dictionary is a string identifier for a specific SACS code block, allowing the framework to manage \textit{both} variable types simultaneously:
        \begin{itemize}
            \item \textbf{Discrete Variables:} Keys such as \texttt{"GRUP\_LG1"}, whose corresponding value is a string containing the complete, fixed-width SACS code line defining a member section selected from a discrete engineering catalog.
            \item \textbf{Continuous Variables:} Keys such as \texttt{"JOINT\_201"}, whose corresponding value is a string containing the \texttt{X}, \texttt{Y}, \texttt{Z} coordinates that define the structure's continuous geometry in $\mathbb{R}^3$.
        \end{itemize}
\end{itemize}
This code-native, mixed-variable representation enables the LLM to directly parse, reason about, and generate syntactically valid code for both variable types within a single, coherent framework. It seamlessly bridges the semantic gap between the high-level optimization logic and the low-level engineering analysis software.

\subsection{The Evaluation Oracle}
A key component, termed the evaluation oracle, serves as the high-fidelity fitness assessment pipeline for a given candidate solution. Its core responsibilities include:
\begin{enumerate}
    \item \textbf{Model Modification:} A file modification module parses the input JSON object and programmatically updates the base SACS model files, substituting the appropriate code blocks with the candidate's specifications.
    \item \textbf{Structural Analysis Execution:} A simulation runner module subsequently invokes the SACS solver to perform a comprehensive structural analysis on the modified design.
    \item \textbf{Objective Function Extraction:} Upon successful completion of the analysis, the oracle parses the simulation outputs to extract the objective function values. The optimization objectives are defined as: (1) minimization of total structural \texttt{weight}, (2) minimization of the maximum axial stress unity check (\texttt{axial\_uc\_max}), and (3) minimization of the maximum bending stress unity check (\texttt{bending\_uc\_max}).
\end{enumerate}
Any candidate design that results in a failed SACS analysis or violates the constraint of any Unity Check (UC) value exceeding 1.0 is deemed infeasible. Such solutions are assigned a prohibitively large penalty fitness score to ensure their removal from the gene pool during the selection phase.

\subsection{Parent Selection Mechanism}
To effectively guide the evolutionary search towards the Pareto front, the LLM-GA implements the NSGA-II selection mechanism. This strategy ensures the preservation of both high-quality solutions (convergence) and a well-distributed set of trade-offs (diversity). The selection process comprises two primary operations:
\begin{itemize}
    \item \textbf{Non-dominated Sorting:} This operation partitions the population into a hierarchy of non-domination fronts. The concept of Pareto dominance is formally defined as follows: for a minimization problem with $M$ objectives, a solution vector $\vec{x}_a$ is said to dominate another vector $\vec{x}_b$ (denoted as $\vec{x}_a \prec \vec{x}_b$) if and only if:
    \begin{equation}
        \forall i \in \{1, \dots, M\}, f_i(\vec{x}_a) \leq f_i(\vec{x}_b) \quad \land \quad \exists j \in \{1, \dots, M\}, f_j(\vec{x}_a) < f_j(\vec{x}_b)
    \end{equation}
    where $f_i(\vec{x})$ is the objective function value for the $i$-th objective. The first front, $F_1$, consists of all solutions that are not dominated by any other solution in the population.

    \item \textbf{Crowding Distance Assignment:} This function calculates a density metric for each solution within its front, which promotes the selection of solutions residing in less-crowded regions of the objective space. The crowding distance $d_k$ for a solution $k$ is calculated by summing the normalized distances of its neighbors along each objective axis:
    \begin{equation}
        d_k = \sum_{m=1}^{M} \frac{f_m(k+1) - f_m(k-1)}{f_m^{\text{max}} - f_m^{\text{min}}}
    \end{equation}
    where $f_m(k+1)$ and $f_m(k-1)$ are the objective values of the solutions adjacent to solution $k$ in the sorted list for objective $m$, and $f_m^{\text{max}}$ and $f_m^{\text{min}}$ are the maximum and minimum values for that objective within the front. Boundary solutions are assigned an infinite distance to ensure their preservation.
\end{itemize}

Parents are selected by giving preference to individuals in lower-indexed fronts and, among individuals in the same front, to those with a greater crowding distance.

\subsection{LLM-driven Genetic Operators: Semantic Crossover and Mutation}
The central innovation of this framework is the substitution of conventional mathematical genetic operators with an LLM-driven, prompt-engineered approach. A dedicated prompt generation module is responsible for the dynamic creation of rich, contextual prompts that guide the LLM's generative process. These prompts are meticulously structured to include:
\begin{itemize}
    \item \textbf{Role-playing Directive:} The LLM is instructed to assume the persona of an expert in SACS structural optimization.
    \item \textbf{Mission Overview:} The prompt provides a comprehensive explanation of the multi-objective optimization task, explicitly detailing the inherent trade-offs between the objectives.
    \item \textbf{Parental Data:} The SACS code blocks and evaluated performance metrics of the selected parent(s) are embedded directly into the prompt, providing concrete data for the LLM to reason from.
    \item \textbf{Explicit Instructions:} A clear command is given to perform either a ``mutation'' or ``crossover'' operation. For mutation, the LLM is directed to analyze a single parent's deficiencies and propose a targeted improvement. For crossover, it is tasked with synergistically combining the strengths of two parents to create a superior offspring.
\end{itemize}

This ``semantic'' approach leverages the LLM's vast, pre-trained knowledge base to perform contextually aware and intelligently directed modifications, transcending the limitations of traditional, numerically-bound operators.

\subsection{The Experience Feedback Loop}
To enable meta-learning and continuous strategy refinement, the framework incorporates a novel experience feedback loop. This mechanism operates as a form of in-context learning, allowing the LLM to improve its generative strategy over the course of an optimization run. The process is as follows:
\begin{enumerate}
    \item \textbf{Knowledge Distillation:} At periodic intervals, the algorithm samples the highest-performing (Pareto-optimal) and lowest-performing solutions from the entire history of evaluated candidates.
    \item \textbf{Strategy Summarization:} A specialized prompt is sent to the LLM, containing these positive and negative exemplars. The LLM is tasked with analyzing their distinguishing features and generating a concise, high-level summary of effective design strategies and common failure modes.
    \item \textbf{Dynamic Prompt Augmentation:} This generated ``experience'' summary is then prepended to all subsequent prompts for mutation and crossover operations.
\end{enumerate}
This feedback mechanism allows the LLM to progressively build a more sophisticated internal model of the optimization problem, learning from its successes and failures to guide the evolutionary search with increasing efficiency and efficacy.




\clearpage
\bibliographystyle{elsarticle-num}
\bibliography{ref}




\end{document}
